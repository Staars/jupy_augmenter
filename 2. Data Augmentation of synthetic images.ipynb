{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Techniques\n",
    "* Scaling\n",
    "* Translation\n",
    "* Rotation (at 90 degrees)\n",
    "* Rotation (at finer angles)\n",
    "* Flipping\n",
    "* Adding Salt and Pepper noise\n",
    "* Lighting condition\n",
    "* Perspective transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q tf-models-nightly --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_models as tfm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from math import floor, ceil, pi\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change image size if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 128\n",
    "\n",
    "path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Resizing\n",
    "def tf_resize_images(X_img_file_paths):\n",
    "    X_data = []\n",
    "        \n",
    "    # Each image is resized individually as different image may be of different size.\n",
    "    # @tf.function\n",
    "    def r(X):\n",
    "        tf_img = tf.image.resize(X, (IMAGE_SIZE, IMAGE_SIZE), \n",
    "                                    tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        return tf_img\n",
    "    \n",
    "    for index, file_path in enumerate(X_img_file_paths):\n",
    "        img = mpimg.imread(file_path)[:, :, :3] # Do not read alpha channel.\n",
    "        resized_img = r(img)\n",
    "        X_data.append(resized_img)\n",
    "\n",
    "    X_data = np.array(X_data, dtype = np.float32) # Convert to numpy\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling:\n",
    "Having differently scaled object of interest in the images is the most important aspect of image diversity. When your network is in hands of real users, the object in the image can be tiny or large. Also, sometimes, object can cover the entire image and yet will not be present totally in image (i.e cropped at edges of object). The code shows scaling of image centrally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_scale_images(X_imgs, scales):\n",
    "    # Various settings needed for Tensorflow operation\n",
    "    boxes = np.zeros((len(scales), 4), dtype = np.float32)\n",
    "    for index, scale in enumerate(scales):\n",
    "        scale = scale\n",
    "        x1 = y1 = 0.5 - 0.5 * scale # To scale centrally\n",
    "        x2 = y2 = 0.5 + 0.5 * scale\n",
    "        boxes[index] = np.array([y1, x1, y2, x2], dtype = np.float32)\n",
    "    box_ind = np.zeros((len(scales)), dtype = np.int32)\n",
    "    crop_size = np.array([IMAGE_SIZE, IMAGE_SIZE], dtype = np.int32)\n",
    "    \n",
    "    X_scale_data = []\n",
    "\n",
    "    # Define Tensorflow operation for all scales but only one base image at a time\n",
    "    # @tf.function\n",
    "    def cr(X):\n",
    "        return tf.image.crop_and_resize(X, boxes, box_ind, crop_size)\n",
    "        \n",
    "    for img_data in X_imgs:\n",
    "        batch_img = np.expand_dims(img_data, axis = 0)\n",
    "        scaled_imgs = cr(batch_img)\n",
    "        X_scale_data.extend(scaled_imgs)\n",
    "    \n",
    "    X_scale_data = np.array(X_scale_data, dtype = np.float32)\n",
    "    return X_scale_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation:\n",
    "We would like our network to recognize the object present in any part of the image. Also, the object can be present partially in the corner or edges of the image. For this reason, we shift the object to various parts of the image. This may also result in addition of a background noise. The code snippet shows translating the image at four sides retaining 80 percent of the base image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "def get_translate_parameters(index):\n",
    "    if index == 0: # Translate left 20 percent\n",
    "        offset = np.array([0.0, 0.2], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.8 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = int(ceil(0.8 * IMAGE_SIZE))\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 1: # Translate right 20 percent\n",
    "        offset = np.array([0.0, -0.2], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.8 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = int(floor((1 - 0.8) * IMAGE_SIZE))\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 2: # Translate top 20 percent\n",
    "        offset = np.array([0.2, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.8 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = int(ceil(0.8 * IMAGE_SIZE)) \n",
    "    elif index == 3: # Translate bottom 20 percent\n",
    "        offset = np.array([-0.2, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.8 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = int(floor((1 - 0.8) * IMAGE_SIZE))\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 4: # Translate left 10 percent\n",
    "        offset = np.array([0.0, 0.1], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.9 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = int(ceil(0.9 * IMAGE_SIZE))\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 5: # Translate right 10 percent\n",
    "        offset = np.array([0.0, -0.1], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.9 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = int(floor((1 - 0.9) * IMAGE_SIZE))\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE \n",
    "    elif index == 6: # Translate top 10 percent\n",
    "        offset = np.array([0.1, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.9 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = int(ceil(0.9 * IMAGE_SIZE))  \n",
    "    elif index == 7: # Translate bottom 10 percent\n",
    "        offset = np.array([-0.1, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.9 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = int(floor((1 - 0.9) * IMAGE_SIZE))\n",
    "        h_end = IMAGE_SIZE      \n",
    "        \n",
    "    return offset, size, w_start, w_end, h_start, h_end\n",
    "\n",
    "def translate_images(X_imgs):\n",
    "    offsets = np.zeros((len(X_imgs), 2), dtype = np.float32)\n",
    "    n_translations = 8\n",
    "    X_translated_arr = []\n",
    "    \n",
    "\n",
    "    for i in range(n_translations):\n",
    "        X_translated = np.zeros((len(X_imgs), IMAGE_SIZE, IMAGE_SIZE, 3), \n",
    "                dtype = np.float32)\n",
    "        X_translated.fill(1.0) # Filling background color\n",
    "        base_offset, size, w_start, w_end, h_start, h_end = get_translate_parameters(i)\n",
    "        offsets[:, :] = base_offset \n",
    "        glimpses = tf.image.extract_glimpse(X_imgs, size, offsets)\n",
    "        \n",
    "        X_translated[:, h_start: h_start + size[0], \\\n",
    "            w_start: w_start + size[1], :] = glimpses\n",
    "        X_translated_arr.extend(X_translated)\n",
    "        \n",
    "    X_translated_arr = np.array(X_translated_arr, dtype = np.float32)\n",
    "    return X_translated_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotation (at 90 degrees):\n",
    "The network has to recognize the object present in any orientation. Assuming the image is square, rotating the image at 90 degrees will not add any background noise in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_images(X_imgs):\n",
    "    X_rotate = []\n",
    "\n",
    "    @tf.function\n",
    "    def rot90(X,k):\n",
    "        return tf.image.rot90(X, k = k)\n",
    "\n",
    "    for img in X_imgs:\n",
    "        for i in range(3):  # Rotation at 90, 180 and 270 degrees\n",
    "            rotated_img = rot90(img, i + 1 )\n",
    "            X_rotate.append(rotated_img)\n",
    "        \n",
    "    X_rotate = np.array(X_rotate, dtype = np.float32)\n",
    "    return X_rotate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotation (at finer angles):\n",
    "Depending upon the requirement, there maybe a necessity to orient the object at minute angles. However problem with this approach is, it will add background noise. If the background in image is of a fixed color (say white or black), the newly added background can blend with the image. However, if the newly added background color doesn’t blend, the network may consider it as to be a feature and learn unnecessary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "def rotate_images(X_imgs, start_angle, end_angle, n_images):\n",
    "    X_rotate = []\n",
    "    iterate_at = (end_angle - start_angle) / (n_images - 1)\n",
    "    \n",
    "    # @tf.function\n",
    "    def rot(X,degrees):\n",
    "        return tfm.vision.augment.rotate(\n",
    "            X, degrees\n",
    "        )\n",
    "    \n",
    "    for index in range(n_images):\n",
    "        degrees_angle = start_angle + index * iterate_at\n",
    "        rotated_imgs = rot( X_imgs, degrees_angle)\n",
    "        X_rotate.extend(rotated_imgs)\n",
    "\n",
    "    X_rotate = np.array(X_rotate, dtype = np.float32)\n",
    "    return X_rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "def shear_images(X_imgs, start_angle, end_angle, n_images):\n",
    "    X_shear = []\n",
    "    iterate_at = (end_angle - start_angle) / (n_images - 1)\n",
    "    \n",
    "    # @tf.function\n",
    "    def shearx(X, degrees):\n",
    "        return tfm.vision.augment.shear_x(\n",
    "            X, degrees, [0,0,0]\n",
    "        )\n",
    "    \n",
    "    for index in range(n_images):\n",
    "        degrees_angle = start_angle + index * iterate_at\n",
    "        rotated_imgs = shearx( X_imgs, degrees_angle)\n",
    "        X_shear.extend(rotated_imgs)\n",
    "\n",
    "    X_shear = np.array(X_shear, dtype = np.float32)\n",
    "    return X_shear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "def gaussian_noise_images(X_imgs, start_angle, end_angle, n_images):\n",
    "    X_gn = []\n",
    "    iterate_at = (end_angle - start_angle) / (n_images - 1)\n",
    "    \n",
    "    # @tf.function\n",
    "    def gn(X, degrees):\n",
    "        return tfm.vision.augment.gaussian_noise(\n",
    "            X, 0.1, degrees\n",
    "        )\n",
    "    \n",
    "    for index in range(n_images):\n",
    "        degrees_angle = start_angle + index * iterate_at\n",
    "        noised_imgs = gn( X_imgs, degrees_angle)\n",
    "        X_gn.extend(noised_imgs)\n",
    "\n",
    "    X_gn = np.array(X_gn, dtype = np.float32)\n",
    "    return X_gn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flipping:\n",
    "This scenario is more important for network to remove biasness of assuming certain features of the object is available in only a particular side. Consider the case shown in image example. You don’t want network to learn that tilt of banana happens only in right side as observed in the base image. Also notice that flipping produces different set of images from rotation at multiple of 90 degrees.My additional question is has anyone done some study on what is the maximum number of classes it gives good performance. Consider, data can be generated with good amount of diversity for each class and time of training is not a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_images(X_imgs):\n",
    "    X_flip = []\n",
    "\n",
    "    @tf.function\n",
    "    def flip(X):\n",
    "        tf_img1 = tf.image.flip_left_right(X)\n",
    "        tf_img2 = tf.image.flip_up_down(X)\n",
    "        tf_img3 = tf.image.transpose(X)\n",
    "        return [tf_img1, tf_img2, tf_img3]\n",
    "\n",
    "    for img in X_imgs:\n",
    "        flipped_imgs = flip(img)\n",
    "        X_flip.extend(flipped_imgs)\n",
    "\n",
    "    X_flip = np.array(X_flip, dtype = np.float32)\n",
    "    return X_flip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Salt and Pepper noise:\n",
    "Salt and Pepper noise refers to addition of white and black dots in the image. Though this may seem unnecessary, it is important to remember that a general user who is taking image to feed into your network may not be a professional photographer. His camera can produce blurry images with lots of white and black dots. This augmentation aides the above mentioned users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_salt_pepper_noise(X_imgs):\n",
    "    # Need to produce a copy as to not modify the original image\n",
    "    X_imgs_copy = X_imgs.copy()\n",
    "    row, col, _ = X_imgs_copy[0].shape\n",
    "    salt_vs_pepper = 0.2\n",
    "    amount = 0.004\n",
    "    num_salt = np.ceil(amount * X_imgs_copy[0].size * salt_vs_pepper)\n",
    "    num_pepper = np.ceil(amount * X_imgs_copy[0].size * (1.0 - salt_vs_pepper))\n",
    "    \n",
    "    for X_img in X_imgs_copy:\n",
    "        # Add Salt noise\n",
    "        coords = [np.random.randint(0, i - 1, int(num_salt)) for i in X_img.shape]\n",
    "        X_img[coords[0], coords[1], :] = 1\n",
    "\n",
    "        # Add Pepper noise\n",
    "        coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in X_img.shape]\n",
    "        X_img[coords[0], coords[1], :] = 0\n",
    "    return X_imgs_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lighting condition:\n",
    "This is a very important type of diversity needed in the image dataset not only for the network to learn properly the object of interest but also to simulate the practical scenario of images being taken by the user. The lighting condition of the images are varied by adding Gaussian noise in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(X_imgs):\n",
    "    gaussian_noise_imgs = []\n",
    "    row, col, _ = X_imgs[0].shape\n",
    "    # Gaussian distribution parameters\n",
    "    mean = 0\n",
    "    var = 0.1\n",
    "    sigma = var ** 0.5\n",
    "    \n",
    "    for X_img in X_imgs:\n",
    "        gaussian = np.random.random((row, col, 1)).astype(np.float32)\n",
    "        gaussian = np.concatenate((gaussian, gaussian, gaussian), axis = 2)\n",
    "        gaussian_img = cv2.addWeighted(X_img, 0.75, 0.25 * gaussian, 0.25, 0)\n",
    "        gaussian_noise_imgs.append(gaussian_img)\n",
    "    gaussian_noise_imgs = np.array(gaussian_noise_imgs, dtype = np.float32)\n",
    "    return gaussian_noise_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perspective transform:\n",
    "In perspective transform, we try to project image from a different point of view. For this, the position of object should be known in advance. Merely calculating perspective transform without knowing the position of the object can lead to degradation of the dataset. Hence, this type of augmentation has to be performed selectively. The greatest advantage with this augmentation is that it can emphasize on parts of object in image which the network needs to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_coord(imshape):\n",
    "    vertices = np.array([[(0.09 * imshape[1], 0.99 * imshape[0]), \n",
    "                          (0.43 * imshape[1], 0.32 * imshape[0]), \n",
    "                          (0.56 * imshape[1], 0.32 * imshape[0]),\n",
    "                          (0.85 * imshape[1], 0.99 * imshape[0])]], dtype = np.int32)\n",
    "    return vertices\n",
    "\n",
    "def get_perspective_matrices(X_img):\n",
    "    offset = 15\n",
    "    img_size = (X_img.shape[1], X_img.shape[0])\n",
    "\n",
    "    # Estimate the coordinates of object of interest inside the image.\n",
    "    src = np.float32(get_mask_coord(X_img.shape))\n",
    "    dst = np.float32([[offset, img_size[1]], [offset, 0], [img_size[0] - offset, 0], \n",
    "                      [img_size[0] - offset, img_size[1]]])\n",
    "    \n",
    "    perspective_matrix = cv2.getPerspectiveTransform(src, dst)\n",
    "    return perspective_matrix\n",
    "\n",
    "def perspective_transform(X_img):\n",
    "    # Doing only for one type of example\n",
    "    perspective_matrix = get_perspective_matrices(X_img)\n",
    "    warped_img = cv2.warpPerspective(X_img, perspective_matrix,\n",
    "                                     (X_img.shape[1], X_img.shape[0]),\n",
    "                                     flags = cv2.INTER_LINEAR)\n",
    "    return warped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_images(X_imgs):\n",
    "    X_inv = []\n",
    "    def invert(X):\n",
    "        return tfm.vision.augment.invert(\n",
    "            tfm.vision.augment.grayscale(\n",
    "                X\n",
    "            )\n",
    "        )\n",
    "    for img in X_imgs:\n",
    "        flipped_imgs = invert(img)\n",
    "        X_inv.append(flipped_imgs)\n",
    "\n",
    "    X_inv = np.array(X_inv, dtype = np.float32)\n",
    "    return X_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(i, X_img):\n",
    "    file_resized = \"img_resized\"\n",
    "    filename_resized = new_img_name(i, file_resized)\n",
    "    tf.keras.utils.save_img(filename_resized,X_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_image(folder, img):\n",
    "    # scale = [0.97,0.96,0.95,0.94,0.93,0.92,0.91,0.90,0.89,0.88,0.87,0.86,0.85,0.84,0.80,0.75,0.70,0.65,0.60]\n",
    "    scale = [0.99,0.98,0.97,0.96,0.95]\n",
    "    scaled_imgs = central_scale_images(img, scale)\n",
    "    for i in range(0, len(scale)):\n",
    "        filename = \"img_scale_{0}\".format(i)\n",
    "        filepath = new_img_name(folder, filename)\n",
    "        #scipy.misc.imsave(filepath, scaled_imgs[i])\n",
    "        tf.keras.utils.save_img(filepath, scaled_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranlate_image(folder, img):\n",
    "    translated_imgs = translate_images(img)\n",
    "    \n",
    "    for i in range(-4, 4):\n",
    "        filename = \"img_translate_{0}\".format(i)\n",
    "        filepath = new_img_name(folder, filename)\n",
    "        # scipy.misc.imsave(filepath, translated_imgs[i])\n",
    "        tf.keras.utils.save_img(filepath, translated_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shear(folder, img):\n",
    "    sheared = shear_images(img, -0.1, 0.1, 14)\n",
    "    \n",
    "    for i in range(14):\n",
    "        filename = \"img_sheared_{0}\".format(i)\n",
    "        filepath = new_img_name(folder, filename)\n",
    "        tf.keras.utils.save_img(filepath, sheared[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_general_image(folder, img):\n",
    "    rotated = rotate_images(img, -10, 10, 14)\n",
    "    \n",
    "    for i in range(14):\n",
    "        filename = \"img_rotated_{0}\".format(i)\n",
    "        filepath = new_img_name(folder, filename)\n",
    "        # scipy.misc.imsave(filepath, rotated[i] )\n",
    "        tf.keras.utils.save_img(filepath, rotated[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipped_image(folder, img):\n",
    "    flipped = flip_images(img)\n",
    "    \n",
    "    for i in range(3):\n",
    "        filename = \"img_flipped_{0}\".format(i)\n",
    "        filepath = new_img_name(folder, filename)\n",
    "        # scipy.misc.imsave(filepath, flipped[i] )\n",
    "        tf.keras.utils.save_img(filepath, flipped[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salt_pepper(folder, img):\n",
    "    salt = add_salt_pepper_noise(img)\n",
    "    filename = \"img_salt_pepper\"\n",
    "    filepath = new_img_name(folder, filename)\n",
    "    # scipy.misc.imsave(filepath, salt[0] )\n",
    "    tf.keras.utils.save_img(filepath, salt[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lighting(folder, img):\n",
    "    gaussian = add_gaussian_noise(img)\n",
    "    filename = \"img_gaussian\"\n",
    "    filepath = new_img_name(folder, filename)\n",
    "    # scipy.misc.imsave(filepath, gaussian[0] )\n",
    "    tf.keras.utils.save_img(filepath, gaussian[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert(folder, img):\n",
    "    inv = invert_images(img)\n",
    "    filename = \"img_inverted\"\n",
    "    filepath = new_img_name(folder, filename)\n",
    "    tf.keras.utils.save_img(filepath, inv[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_noise(folder, img):\n",
    "    g_n = gaussian_noise_images(img, 0.1, 2.0, 14)\n",
    "    for i in range(14):\n",
    "        filename = \"img_gaussian_noise_{0}\".format(i)\n",
    "        filepath = new_img_name(folder, filename)\n",
    "        tf.keras.utils.save_img(filepath, g_n[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_images(i, img):\n",
    "    # # Resized Image\n",
    "    # resize_image(i, img)\n",
    "    # Scale 97% - 60%\n",
    "    scaled_image(i, img)\n",
    "    # Tranlate the images\n",
    "    tranlate_image(i, img)\n",
    "    # # Rotate the image 180, 270 degress\n",
    "    rotate_general_image(i, img)\n",
    "    # Shear the image\n",
    "    shear(i, img)\n",
    "    # # Flip the image\n",
    "    # flipped_image(i, img)\n",
    "    # Add noise in pixels\n",
    "    salt_pepper(i, img)\n",
    "    # Add gaussian noise in pixels\n",
    "    gaussian_noise(i, img)\n",
    "    # Lighting condition\n",
    "    lighting(i, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_img_name(full_path, new_name):\n",
    "        new_path = full_path.split(\"/\")\n",
    "        old_name = new_path.pop()\n",
    "        old_name = old_name.split(\".\")[0] + \"_\"\n",
    "        new_path = \"/\".join(new_path) + \"/\"\n",
    "        filepath = path.format(new_path, old_name + new_name)\n",
    "        return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirList = glob.glob(\"./augmented/*/*.png\")\n",
    "X_img_paths = ['{}'.format(file) for file in dirList]\n",
    "# filename = 'img_0.png'\n",
    "# path = ''\n",
    "\n",
    "for img in X_img_paths:\n",
    "        path = \"{0}{1}.\" + img.split(\".\")[-1] #  img.split(\".\")[0] + \n",
    "        # print(path)\n",
    "        X_resize = tf_resize_images([img])\n",
    "        exec_images(img, X_resize)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create inverted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirList = glob.glob(\"./augmented/*/*.png\")\n",
    "X_img_paths = ['{}'.format(file) for file in dirList]\n",
    "\n",
    "for img in X_img_paths:\n",
    "        path = img.split(\".\")[0] + \"{0}{1}.\" + img.split(\".\")[-1]\n",
    "        X_resize = tf_resize_images([img])\n",
    "        # Invert Image\n",
    "        invert(img, X_resize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
